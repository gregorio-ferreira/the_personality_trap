{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b979afd",
   "metadata": {},
   "source": [
    "# Questionnaire Experiments - Research Replication Guide\n",
    "\n",
    "This notebook demonstrates the questionnaire evaluation workflow from **\"The Personality Trap\"** research, continuing from the persona generation step.\n",
    "\n",
    "## Overview\n",
    "\n",
    "After generating AI personas, this workflow:\n",
    "\n",
    "1. **Experiment Registration**: Create experiment groups and register individual runs\n",
    "2. **Questionnaire Administration**: LLMs impersonate personas and answer personality questionnaires\n",
    "3. **Response Logging**: Track all LLM requests/responses in the database\n",
    "4. **Answer Processing**: Store and validate questionnaire answers\n",
    "5. **Verification**: Inspect experiment results and data quality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "✅ **Complete `personas_generation.ipynb` first** - you need generated personas to run experiments\n",
    "\n",
    "✅ Database schema initialized with:\n",
    "- `personas` table populated with generated personas\n",
    "- `experiments_groups` table ready for experiment organization\n",
    "- `experiments_list` table ready for individual experiment tracking\n",
    "- `eval_questionnaires` table ready for storing LLM answers\n",
    "- `experiment_request_metadata` table ready for API logs\n",
    "\n",
    "✅ LLM API keys configured (same as persona generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Database Access and Imports\n",
    "\n",
    "Set up the environment for questionnaire experiments. This cell:\n",
    "\n",
    "- Imports required libraries for experiment registration and execution\n",
    "- Establishes database connection to your experimental schema\n",
    "- Configures logging to track experiment progress\n",
    "- Automatically loads configuration from `.yaml` in project root\n",
    "\n",
    "**Note:** This assumes you've already run `personas_generation.ipynb` and have personas in your experimental schema.\n",
    "\n",
    "**Database Architecture:**\n",
    "The questionnaire experiment system uses two main tables:\n",
    "1. `experiments_groups`: Defines batches of related experiments (what + how)\n",
    "2. `experiments_list`: Individual experiment runs (who + results)\n",
    "\n",
    "Each experiment in `experiments_list` links to its parent group via `experiments_group_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:22:58,644 - personas_backend.db.db_handler - INFO - Connected to the database!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "🧪 Experimental schema: my_schema_v00\n",
      "📊 Database connection established\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Import configuration and core modules\n",
    "from personas_backend.utils.config import ConfigManager\n",
    "from personas_backend.core.enums import ModelID\n",
    "from personas_backend.db.db_handler import DatabaseHandler\n",
    "from personas_backend.evaluate_questionnaire import (\n",
    "    register_questionnaire_experiments,\n",
    "    run_pending_experiments\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load configuration (automatically finds .yaml in project root)\n",
    "config = ConfigManager()\n",
    "\n",
    "# Initialize database handler\n",
    "db_handler = DatabaseHandler(config_manager=config)\n",
    "\n",
    "# Get experimental schema from YAML config\n",
    "EXPERIMENTAL_SCHEMA = config.schema_config.target_schema\n",
    "\n",
    "print(f\"✅ Configuration loaded successfully\")\n",
    "print(f\"🧪 Experimental schema: {EXPERIMENTAL_SCHEMA}\")\n",
    "print(f\"📊 Database connection established\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c85705",
   "metadata": {},
   "source": [
    "## Step 2: Verify Generated Personas\n",
    "\n",
    "Before registering experiments, confirm that personas exist in your experimental schema. This validation:\n",
    "\n",
    "- Counts personas by model and population\n",
    "- Identifies available `ref_personality_id` values for experiments\n",
    "- Ensures the prerequisite data from `personas_generation.ipynb` is present\n",
    "\n",
    "**If this cell shows no personas:** Run `personas_generation.ipynb` first to generate AI personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68efb55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_personas",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_models",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_populations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_personalities",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ca012064-0bac-4be1-a280-8916cdae2598",
       "rows": [
        [
         "0",
         "6",
         "2",
         "3",
         "2"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_personas</th>\n",
       "      <th>unique_models</th>\n",
       "      <th>unique_populations</th>\n",
       "      <th>unique_personalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_personas  unique_models  unique_populations  unique_personalities\n",
       "0               6              2                   3                     2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 6 personas\n",
      "\n",
      "📊 Breakdown by model and population:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "be6c80c9-ba22-4880-92fe-b9e1e276d2c1",
       "rows": [
        [
         "0",
         "gpt35",
         "generated_gpt35_spain826",
         "2"
        ],
        [
         "1",
         "gpt4o",
         "borderline_maxN_gpt4o",
         "2"
        ],
        [
         "2",
         "gpt4o",
         "borderline_maxP_gpt4o",
         "2"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                population  count\n",
       "0  gpt35  generated_gpt35_spain826      2\n",
       "1  gpt4o     borderline_maxN_gpt4o      2\n",
       "2  gpt4o     borderline_maxP_gpt4o      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Available personality IDs for experiments: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Check for existing personas\n",
    "personas_query = f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_personas,\n",
    "        COUNT(DISTINCT model) as unique_models,\n",
    "        COUNT(DISTINCT population) as unique_populations,\n",
    "        COUNT(DISTINCT ref_personality_id) as unique_personalities\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.personas\n",
    "\"\"\"\n",
    "\n",
    "personas_summary = pd.read_sql_query(personas_query, db_handler.connection)\n",
    "display(personas_summary)\n",
    "\n",
    "if personas_summary['total_personas'].iloc[0] == 0:\n",
    "    raise ValueError(\n",
    "        f\"No personas found in {EXPERIMENTAL_SCHEMA}.personas! \"\n",
    "        f\"Run 'personas_generation.ipynb' first to generate personas.\"\n",
    "    )\n",
    "\n",
    "# Get detailed breakdown\n",
    "personas_breakdown_query = f\"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        population,\n",
    "        COUNT(*) as count\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.personas\n",
    "    GROUP BY model, population\n",
    "    ORDER BY model, population\n",
    "\"\"\"\n",
    "\n",
    "personas_breakdown = pd.read_sql_query(personas_breakdown_query, db_handler.connection)\n",
    "\n",
    "print(f\"\\n✅ Found {personas_summary['total_personas'].iloc[0]} personas\")\n",
    "print(f\"\\n📊 Breakdown by model and population:\")\n",
    "display(personas_breakdown)\n",
    "\n",
    "# Extract available personality IDs for experiment registration\n",
    "available_personality_ids_query = f\"\"\"\n",
    "    SELECT DISTINCT ref_personality_id\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.personas\n",
    "    WHERE ref_personality_id IS NOT NULL\n",
    "    ORDER BY ref_personality_id\n",
    "\"\"\"\n",
    "\n",
    "personality_ids_df = pd.read_sql_query(available_personality_ids_query, db_handler.connection)\n",
    "available_personality_ids = personality_ids_df['ref_personality_id'].tolist()\n",
    "\n",
    "print(f\"\\n🎯 Available personality IDs for experiments: {available_personality_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Auto-Discover and Register All Experiment Groups\n",
    "\n",
    "**Automated Multi-Model Experiment Registration**\n",
    "\n",
    "Instead of manually specifying models and populations, this step **automatically discovers** all generated personas and creates experiment groups for each unique model-population combination.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "**Phase 1: Discovery**\n",
    "- Query the `personas` table to find all unique (model, population) combinations\n",
    "- Count personas in each combination\n",
    "- Display a summary of what will be processed\n",
    "\n",
    "**Phase 2: Batch Registration**\n",
    "For each discovered model-population pair:\n",
    "1. Create an experiment group in `experiments_groups` table\n",
    "2. Register individual experiments in `experiments_list` (one per persona)\n",
    "3. Track all group IDs and experiment IDs for later execution\n",
    "\n",
    "### Architecture Pattern (Multi-Group)\n",
    "```\n",
    "experiments_groups\n",
    "    ├── Group 1: gpt4o / generated_gpt4o_spain826 (baseline)\n",
    "    │   ├── experiment_id: 1001, 1002, 1003...\n",
    "    ├── Group 2: gpt35 / generated_gpt35_spain826 (baseline)  \n",
    "    │   ├── experiment_id: 2001, 2002, 2003...\n",
    "    ├── Group 3: gpt4o / borderline_maxN_gpt4o\n",
    "    │   ├── experiment_id: 3001, 3002, 3003...\n",
    "    └── Group 4: gpt4o / borderline_maxP_gpt4o\n",
    "        └── experiment_id: 4001, 4002, 4003...\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- ✅ **No manual configuration** - automatically processes all generated personas\n",
    "- ✅ **Handles baseline AND borderline** populations seamlessly\n",
    "- ✅ **Supports multiple models** - GPT-3.5, GPT-4o, Claude, Llama, etc.\n",
    "- ✅ **Fault tolerant** - continues if one registration fails\n",
    "\n",
    "### Key Variables:\n",
    "- `QUESTIONNAIRE_TYPE`: Which questionnaire to administer ('bigfive' or 'epqra')\n",
    "- `ALL_EXPERIMENT_GROUPS`: List of all created group metadata\n",
    "- `ALL_REGISTERED_EXPERIMENTS`: Complete list of experiment IDs across all groups\n",
    "\n",
    "**Note**: Each group links to its specific model-population combination, ensuring experiments are run with the correct LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Discovering all generated persona populations in my_schema_v00...\n",
      "\n",
      "✅ Found 3 model-population combinations:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "persona_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_personalities",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "566efe7c-3796-46f8-91d8-04738e6addc2",
       "rows": [
        [
         "0",
         "gpt35",
         "generated_gpt35_spain826",
         "2",
         "2"
        ],
        [
         "1",
         "gpt4o",
         "borderline_maxN_gpt4o",
         "2",
         "2"
        ],
        [
         "2",
         "gpt4o",
         "borderline_maxP_gpt4o",
         "2",
         "2"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>persona_count</th>\n",
       "      <th>unique_personalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                population  persona_count  unique_personalities\n",
       "0  gpt35  generated_gpt35_spain826              2                     2\n",
       "1  gpt4o     borderline_maxN_gpt4o              2                     2\n",
       "2  gpt4o     borderline_maxP_gpt4o              2                     2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Experiment Configuration:\n",
      "   • Questionnaire: epqra\n",
      "   • Schema: my_schema_v00\n",
      "   • Total populations to process: 3\n",
      "\n",
      "⏳ Registering experiment groups for all populations...\n",
      "\n",
      "📋 Processing: gpt35 / generated_gpt35_spain826 (2 personas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:22:59,032 - personas_backend.evaluate_questionnaire.registration - INFO - Loaded 2 personas from schema=my_schema_v00 for model=gpt35 populations=['generated_gpt35_spain826']\n",
      "2025-10-09 18:22:59,090 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,110 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment group with ID: 1 in schema: my_schema_v00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:22:59,181 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,223 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=1 in schema=my_schema_v00\n",
      "2025-10-09 18:22:59,239 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=2 in schema=my_schema_v00\n",
      "2025-10-09 18:22:59,250 - personas_backend.evaluate_questionnaire.registration - INFO - Loaded 2 personas from schema=my_schema_v00 for model=gpt4o populations=['borderline_maxN_gpt4o']\n",
      "2025-10-09 18:22:59,308 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,321 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment group with ID: 2 in schema: my_schema_v00\n",
      "2025-10-09 18:22:59,379 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,396 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=3 in schema=my_schema_v00\n",
      "2025-10-09 18:22:59,417 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=4 in schema=my_schema_v00\n",
      "2025-10-09 18:22:59,428 - personas_backend.evaluate_questionnaire.registration - INFO - Loaded 2 personas from schema=my_schema_v00 for model=gpt4o populations=['borderline_maxP_gpt4o']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Group 1: 2 experiments registered\n",
      "\n",
      "📋 Processing: gpt4o / borderline_maxN_gpt4o (2 personas)\n",
      "   ✅ Group 2: 2 experiments registered\n",
      "\n",
      "📋 Processing: gpt4o / borderline_maxP_gpt4o (2 personas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:22:59,513 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,525 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment group with ID: 3 in schema: my_schema_v00\n",
      "2025-10-09 18:22:59,629 - personas_backend.evaluate_questionnaire.registration - INFO - Connected to the database!\n",
      "2025-10-09 18:22:59,670 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=5 in schema=my_schema_v00\n",
      "2025-10-09 18:22:59,712 - personas_backend.evaluate_questionnaire.registration - INFO - Registered new experiment id=6 in schema=my_schema_v00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Group 3: 2 experiments registered\n",
      "\n",
      "🎉 Registration Summary:\n",
      "   • Total experiment groups created: 3\n",
      "   • Total experiments registered: 6\n",
      "\n",
      "📊 Experiment Groups:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "experiment_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c42ede39-9157-4555-adaf-dcc8a956cfe6",
       "rows": [
        [
         "0",
         "1",
         "gpt35",
         "generated_gpt35_spain826",
         "2"
        ],
        [
         "1",
         "2",
         "gpt4o",
         "borderline_maxN_gpt4o",
         "2"
        ],
        [
         "2",
         "3",
         "gpt4o",
         "borderline_maxP_gpt4o",
         "2"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>experiment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>gpt35</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>gpt4o</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group_id  model                population  experiment_count\n",
       "0         1  gpt35  generated_gpt35_spain826                 2\n",
       "1         2  gpt4o     borderline_maxN_gpt4o                 2\n",
       "2         3  gpt4o     borderline_maxP_gpt4o                 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3.1: Discover all generated persona populations automatically\n",
    "print(f\"🔍 Discovering all generated persona populations in {EXPERIMENTAL_SCHEMA}...\")\n",
    "\n",
    "# Query to get all unique model-population combinations\n",
    "discovery_query = f\"\"\"\n",
    "    SELECT \n",
    "        model,\n",
    "        population,\n",
    "        COUNT(*) as persona_count,\n",
    "        COUNT(DISTINCT ref_personality_id) as unique_personalities\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.personas\n",
    "    GROUP BY model, population\n",
    "    ORDER BY model, population\n",
    "\"\"\"\n",
    "\n",
    "populations_df = pd.read_sql_query(discovery_query, db_handler.connection)\n",
    "\n",
    "if populations_df.empty:\n",
    "    raise ValueError(\n",
    "        f\"No personas found in {EXPERIMENTAL_SCHEMA}.personas! \"\n",
    "        f\"Run 'personas_generation.ipynb' first to generate personas.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Found {len(populations_df)} model-population combinations:\")\n",
    "display(populations_df)\n",
    "\n",
    "# Step 3.2: Configure experiment parameters\n",
    "QUESTIONNAIRE_TYPE = \"epqra\"  # Options: 'bigfive', 'epqra'\n",
    "\n",
    "print(f\"\\n🎯 Experiment Configuration:\")\n",
    "print(f\"   • Questionnaire: {QUESTIONNAIRE_TYPE}\")\n",
    "print(f\"   • Schema: {EXPERIMENTAL_SCHEMA}\")\n",
    "print(f\"   • Total populations to process: {len(populations_df)}\")\n",
    "\n",
    "# Step 3.3: Register experiment groups for ALL model-population combinations\n",
    "print(f\"\\n⏳ Registering experiment groups for all populations...\")\n",
    "\n",
    "# Store all experiment group IDs and experiment IDs\n",
    "ALL_EXPERIMENT_GROUPS = []\n",
    "ALL_REGISTERED_EXPERIMENTS = []\n",
    "\n",
    "for idx, row in populations_df.iterrows():\n",
    "    model = row['model']\n",
    "    population = row['population']\n",
    "    persona_count = row['persona_count']\n",
    "    \n",
    "    print(f\"\\n📋 Processing: {model} / {population} ({persona_count} personas)\")\n",
    "    \n",
    "    try:\n",
    "        group_id, experiment_ids = register_questionnaire_experiments(\n",
    "            questionnaire=QUESTIONNAIRE_TYPE,\n",
    "            model=model,\n",
    "            populations=[population],\n",
    "            experiment_description=f\"Replication: {QUESTIONNAIRE_TYPE} for {model} / {population}\",\n",
    "            schema=EXPERIMENTAL_SCHEMA,\n",
    "        )\n",
    "        \n",
    "        ALL_EXPERIMENT_GROUPS.append({\n",
    "            'group_id': group_id,\n",
    "            'model': model,\n",
    "            'population': population,\n",
    "            'experiment_count': len(experiment_ids)\n",
    "        })\n",
    "        ALL_REGISTERED_EXPERIMENTS.extend(experiment_ids)\n",
    "        \n",
    "        print(f\"   ✅ Group {group_id}: {len(experiment_ids)} experiments registered\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Failed to register {model}/{population}: {e}\")\n",
    "        # Continue with other populations even if one fails\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "groups_summary_df = pd.DataFrame(ALL_EXPERIMENT_GROUPS)\n",
    "\n",
    "print(f\"\\n🎉 Registration Summary:\")\n",
    "print(f\"   • Total experiment groups created: {len(ALL_EXPERIMENT_GROUPS)}\")\n",
    "print(f\"   • Total experiments registered: {len(ALL_REGISTERED_EXPERIMENTS)}\")\n",
    "print(f\"\\n📊 Experiment Groups:\")\n",
    "display(groups_summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede0686",
   "metadata": {},
   "source": [
    "## Step 3.5: Understanding Repetitions and Experiment Architecture\n",
    "\n",
    "**How repetitions work:**\n",
    "\n",
    "### Persona-Level Repetitions (Automatic)\n",
    "Each persona in the database has a `repetitions` field (default: 1) that indicates how many times to repeat experiments for that persona. This is used for statistical reliability testing.\n",
    "\n",
    "**Example:**\n",
    "- Persona #1 has `repetitions = 3`\n",
    "- Persona #2 has `repetitions = 1`\n",
    "- Registration creates: 4 experiments total (3 for persona #1, 1 for persona #2)\n",
    "\n",
    "### Registration Flow\n",
    "```python\n",
    "register_questionnaire_experiments(\n",
    "    questionnaire=\"bigfive\",\n",
    "    model=\"gpt4o\",\n",
    "    populations=[\"generated_gpt4o_spain826\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Fetch personas**: Query `personas` table filtered by model + populations\n",
    "2. **Create experiment group**: One record in `experiments_groups` with:\n",
    "   - Description, system_role, base_prompt (questionnaire JSON)\n",
    "   - LLM parameters (temperature, top_p)\n",
    "   - Metadata (created timestamp, git SHA)\n",
    "3. **Register experiments**: For each persona:\n",
    "   - Read `persona.repetitions` (how many times to repeat)\n",
    "   - Create N experiments in `experiments_list` (one per repetition)\n",
    "   - Each links to group via `experiments_group_id`\n",
    "4. **Return**: `(group_id, list_of_experiment_ids)`\n",
    "\n",
    "### Database Relationships\n",
    "```\n",
    "experiments_groups (1 record)\n",
    "    ├── experiments_list (N records, N = sum of all persona.repetitions)\n",
    "    │   ├── experiment_id: 1001 (persona=42, repetition=1)\n",
    "    │   ├── experiment_id: 1002 (persona=42, repetition=2)  \n",
    "    │   ├── experiment_id: 1003 (persona=42, repetition=3)\n",
    "    │   └── experiment_id: 1004 (persona=99, repetition=1)\n",
    "```\n",
    "\n",
    "**For this research replication:**\n",
    "- All baseline personas have `repetitions = 1` (one experiment per persona)\n",
    "- Borderline personas also have `repetitions = 1`\n",
    "- Advanced users can set `max_repetitions` parameter to cap experiments (useful for testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify All Experiment Group Registrations\n",
    "\n",
    "Inspect the registered experiment groups before execution. This validation shows:\n",
    "\n",
    "- **All experiment groups** created across different models and populations\n",
    "- Group metadata (description, parameters, prompts)\n",
    "- Individual experiment records per group\n",
    "- Current status (should all be `succeeded=NULL` for pending experiments)\n",
    "\n",
    "This comprehensive view lets you:\n",
    "- Confirm all model-population combinations were registered\n",
    "- Verify experiment counts match expectations\n",
    "- Review the architecture before running expensive LLM API calls\n",
    "\n",
    "**What to check:**\n",
    "- Each model-population pair has its own experiment group\n",
    "- Experiment count per group = number of personas in that population\n",
    "- Baseline AND borderline populations are represented (if generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1bc6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Experiment Group → Experiments Relationship:\n",
      "   (Showing most recent 5 groups)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments_group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "description_preview",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "temperature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "top_p",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_experiments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_populations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unique_personas",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "be872737-36bd-40d6-8d56-7e40eac49721",
       "rows": [
        [
         "0",
         "3",
         "Replication: epqra for gpt4o / borderline_maxP_gpt...",
         "1.0",
         "1.0",
         "2",
         "1",
         "2"
        ],
        [
         "1",
         "2",
         "Replication: epqra for gpt4o / borderline_maxN_gpt...",
         "1.0",
         "1.0",
         "2",
         "1",
         "2"
        ],
        [
         "2",
         "1",
         "Replication: epqra for gpt35 / generated_gpt35_spa...",
         "1.0",
         "1.0",
         "2",
         "1",
         "2"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>description_preview</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>total_experiments</th>\n",
       "      <th>unique_populations</th>\n",
       "      <th>unique_personas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Replication: epqra for gpt4o / borderline_maxP...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Replication: epqra for gpt4o / borderline_maxN...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Replication: epqra for gpt35 / generated_gpt35...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiments_group_id                                description_preview  \\\n",
       "0                     3  Replication: epqra for gpt4o / borderline_maxP...   \n",
       "1                     2  Replication: epqra for gpt4o / borderline_maxN...   \n",
       "2                     1  Replication: epqra for gpt35 / generated_gpt35...   \n",
       "\n",
       "   temperature  top_p  total_experiments  unique_populations  unique_personas  \n",
       "0          1.0    1.0                  2                   1                2  \n",
       "1          1.0    1.0                  2                   1                2  \n",
       "2          1.0    1.0                  2                   1                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Interpretation:\n",
      "   • Each row = 1 experiment group\n",
      "   • total_experiments = number of individual experiment runs\n",
      "   • unique_personas = distinct personalities being tested\n",
      "   • Relationship: 1 group has N experiments (one per persona)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Explore the relationship between experiment groups and experiments\n",
    "SHOW_ARCHITECTURE = True  # Set to True to see how groups and experiments relate\n",
    "\n",
    "if SHOW_ARCHITECTURE:\n",
    "    # Query to show the structure\n",
    "    architecture_query = f\"\"\"\n",
    "        SELECT \n",
    "            eg.experiments_group_id,\n",
    "            LEFT(eg.description, 50) || '...' as description_preview,\n",
    "            eg.temperature,\n",
    "            eg.top_p,\n",
    "            COUNT(el.experiment_id) as total_experiments,\n",
    "            COUNT(DISTINCT el.population) as unique_populations,\n",
    "            COUNT(DISTINCT el.personality_id) as unique_personas\n",
    "        FROM {EXPERIMENTAL_SCHEMA}.experiments_groups eg\n",
    "        LEFT JOIN {EXPERIMENTAL_SCHEMA}.experiments_list el \n",
    "            ON eg.experiments_group_id = el.experiments_group_id\n",
    "        GROUP BY eg.experiments_group_id, eg.description, eg.temperature, eg.top_p\n",
    "        ORDER BY eg.experiments_group_id DESC\n",
    "        LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    architecture_df = pd.read_sql_query(architecture_query, db_handler.connection)\n",
    "    \n",
    "    print(f\"📊 Experiment Group → Experiments Relationship:\")\n",
    "    print(f\"   (Showing most recent 5 groups)\\n\")\n",
    "    display(architecture_df)\n",
    "    \n",
    "    print(f\"\\n💡 Interpretation:\")\n",
    "    print(f\"   • Each row = 1 experiment group\")\n",
    "    print(f\"   • total_experiments = number of individual experiment runs\")\n",
    "    print(f\"   • unique_personas = distinct personalities being tested\")\n",
    "    print(f\"   • Relationship: 1 group has N experiments (one per persona)\")\n",
    "else:\n",
    "    print(f\"ℹ️  Architecture exploration skipped.\")\n",
    "    print(f\"   Set SHOW_ARCHITECTURE=True to see experiment group structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 All Registered Experiment Groups (3):\n",
      "\n",
      "Showing key fields (transpose for readability):\n",
      "\n",
      "================================================================================\n",
      "Group 1: Replication: epqra for gpt35 / generated_gpt35_spain826...\n",
      "   • Temperature: 1.0\n",
      "   • Top-p: 1.0\n",
      "   • Created: 2025-10-09T17:22:59.093105\n",
      "================================================================================\n",
      "Group 2: Replication: epqra for gpt4o / borderline_maxN_gpt4o...\n",
      "   • Temperature: 1.0\n",
      "   • Top-p: 1.0\n",
      "   • Created: 2025-10-09T17:22:59.310968\n",
      "================================================================================\n",
      "Group 3: Replication: epqra for gpt4o / borderline_maxP_gpt4o...\n",
      "   • Temperature: 1.0\n",
      "   • Top-p: 1.0\n",
      "   • Created: 2025-10-09T17:22:59.515859\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📊 All Registered Experiments (showing first 20):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments_group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "questionnaire",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "personality_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "repeated",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "succeeded",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "created",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "34197806-bee8-499b-97dd-8bd60a8f6693",
       "rows": [
        [
         "0",
         "1",
         "1",
         "epqra",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "1",
         "1",
         null,
         "2025-10-09T17:22:59.207402"
        ],
        [
         "1",
         "2",
         "1",
         "epqra",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "2",
         "1",
         null,
         "2025-10-09T17:22:59.231279"
        ],
        [
         "2",
         "3",
         "2",
         "epqra",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "2",
         "1",
         null,
         "2025-10-09T17:22:59.386591"
        ],
        [
         "3",
         "4",
         "2",
         "epqra",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "1",
         "1",
         null,
         "2025-10-09T17:22:59.406758"
        ],
        [
         "4",
         "5",
         "3",
         "epqra",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "2",
         "1",
         null,
         "2025-10-09T17:22:59.639785"
        ],
        [
         "5",
         "6",
         "3",
         "epqra",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "1",
         "1",
         null,
         "2025-10-09T17:22:59.689474"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>personality_id</th>\n",
       "      <th>repeated</th>\n",
       "      <th>succeeded</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.207402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.231279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.386591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.406758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.639785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>epqra</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-10-09T17:22:59.689474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id  experiments_group_id questionnaire               model  \\\n",
       "0              1                     1         epqra  gpt-3.5-turbo-0125   \n",
       "1              2                     1         epqra  gpt-3.5-turbo-0125   \n",
       "2              3                     2         epqra   gpt-4o-2024-11-20   \n",
       "3              4                     2         epqra   gpt-4o-2024-11-20   \n",
       "4              5                     3         epqra   gpt-4o-2024-11-20   \n",
       "5              6                     3         epqra   gpt-4o-2024-11-20   \n",
       "\n",
       "                 population  personality_id  repeated succeeded  \\\n",
       "0  generated_gpt35_spain826               1         1      None   \n",
       "1  generated_gpt35_spain826               2         1      None   \n",
       "2     borderline_maxN_gpt4o               2         1      None   \n",
       "3     borderline_maxN_gpt4o               1         1      None   \n",
       "4     borderline_maxP_gpt4o               2         1      None   \n",
       "5     borderline_maxP_gpt4o               1         1      None   \n",
       "\n",
       "                      created  \n",
       "0  2025-10-09T17:22:59.207402  \n",
       "1  2025-10-09T17:22:59.231279  \n",
       "2  2025-10-09T17:22:59.386591  \n",
       "3  2025-10-09T17:22:59.406758  \n",
       "4  2025-10-09T17:22:59.639785  \n",
       "5  2025-10-09T17:22:59.689474  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Summary by Experiment Group:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('experiments_group_id', 'model', 'population')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "total_experiments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "succeeded",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pending",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef2cc3c4-12ab-4d85-9e64-029dc49a99c1",
       "rows": [
        [
         "(np.int64(1), 'gpt-3.5-turbo-0125', 'generated_gpt35_spain826')",
         "2",
         "0",
         "2"
        ],
        [
         "(np.int64(2), 'gpt-4o-2024-11-20', 'borderline_maxN_gpt4o')",
         "2",
         "0",
         "2"
        ],
        [
         "(np.int64(3), 'gpt-4o-2024-11-20', 'borderline_maxP_gpt4o')",
         "2",
         "0",
         "2"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_experiments</th>\n",
       "      <th>succeeded</th>\n",
       "      <th>pending</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <th>generated_gpt35_spain826</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxN_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxP_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  total_experiments  \\\n",
       "experiments_group_id model              population                                    \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826                  2   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o                     2   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o                     2   \n",
       "\n",
       "                                                                  succeeded  \\\n",
       "experiments_group_id model              population                            \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826          0   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o             0   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o             0   \n",
       "\n",
       "                                                                  pending  \n",
       "experiments_group_id model              population                         \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826        2  \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o           2  \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Registration Summary:\n",
      "   • Total experiment groups: 3\n",
      "   • Total experiments registered: 6\n",
      "   • Pending (not yet run): 6\n",
      "   • Succeeded: 0\n",
      "   • Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Inspect ALL experiment groups in detail\n",
    "all_group_ids_str = ', '.join(str(g['group_id']) for g in ALL_EXPERIMENT_GROUPS)\n",
    "\n",
    "# Query all experiment groups\n",
    "groups_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.experiments_groups\n",
    "    WHERE experiments_group_id IN ({all_group_ids_str})\n",
    "    ORDER BY experiments_group_id\n",
    "\"\"\"\n",
    "\n",
    "all_groups_df = pd.read_sql_query(groups_query, db_handler.connection)\n",
    "\n",
    "print(f\"📋 All Registered Experiment Groups ({len(all_groups_df)}):\")\n",
    "print(f\"\\nShowing key fields (transpose for readability):\\n\")\n",
    "\n",
    "# Display each group's key info\n",
    "for idx, row in all_groups_df.iterrows():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Group {row['experiments_group_id']}: {row.get('description', 'N/A')[:60]}...\")\n",
    "    print(f\"   • Temperature: {row.get('temperature', 'N/A')}\")\n",
    "    print(f\"   • Top-p: {row.get('top_p', 'N/A')}\")\n",
    "    print(f\"   • Created: {row.get('created', 'N/A')}\")\n",
    "\n",
    "# Query all registered experiments across all groups\n",
    "experiments_query = f\"\"\"\n",
    "    SELECT\n",
    "        experiment_id,\n",
    "        experiments_group_id,\n",
    "        questionnaire,\n",
    "        model,\n",
    "        population,\n",
    "        personality_id,\n",
    "        repeated,\n",
    "        succeeded,\n",
    "        created\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.experiments_list\n",
    "    WHERE experiments_group_id IN ({all_group_ids_str})\n",
    "    ORDER BY experiments_group_id, experiment_id\n",
    "\"\"\"\n",
    "\n",
    "all_experiments_df = pd.read_sql_query(experiments_query, db_handler.connection)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\\n📊 All Registered Experiments (showing first 20):\")\n",
    "display(all_experiments_df.head(20))\n",
    "\n",
    "# Summary by group\n",
    "print(f\"\\n📈 Summary by Experiment Group:\")\n",
    "group_exp_summary = all_experiments_df.groupby(['experiments_group_id', 'model', 'population']).agg({\n",
    "    'experiment_id': 'count',\n",
    "    'succeeded': lambda x: (x == True).sum()\n",
    "}).rename(columns={'experiment_id': 'total_experiments', 'succeeded': 'succeeded'})\n",
    "group_exp_summary['pending'] = all_experiments_df.groupby(['experiments_group_id', 'model', 'population'])['succeeded'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "display(group_exp_summary)\n",
    "\n",
    "print(f\"\\n✅ Overall Registration Summary:\")\n",
    "print(f\"   • Total experiment groups: {len(all_groups_df)}\")\n",
    "print(f\"   • Total experiments registered: {len(all_experiments_df)}\")\n",
    "print(f\"   • Pending (not yet run): {all_experiments_df['succeeded'].isna().sum()}\")\n",
    "print(f\"   • Succeeded: {(all_experiments_df['succeeded'] == True).sum()}\")\n",
    "print(f\"   • Failed: {(all_experiments_df['succeeded'] == False).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute ALL Pending Experiments Across All Groups\n",
    "\n",
    "**Multi-Model Batch Execution** - This runs experiments for ALL model-population combinations simultaneously.\n",
    "\n",
    "**What happens during execution:**\n",
    "\n",
    "1. **For each experiment group:**\n",
    "   - System processes experiments using the group's assigned model (GPT-4o, GPT-3.5, Claude, etc.)\n",
    "   - Each experiment impersonates its specific persona\n",
    "   - Questionnaire questions are sent sequentially to the appropriate LLM\n",
    "   - Answers are parsed and validated (1-5 scale)\n",
    "\n",
    "2. **Concurrent processing:**\n",
    "   - Multiple experiments run in parallel (controlled by `max_workers`)\n",
    "   - Different models can be queried simultaneously\n",
    "   - Progress logged in real-time for all groups\n",
    "\n",
    "3. **Data logging (automatic for all groups):**\n",
    "   - `experiment_request_metadata`: Full LLM API request/response JSON (per model)\n",
    "   - `eval_questionnaires`: Parsed questionnaire answers\n",
    "   - `experiments_list`: Status updates with `succeeded` flags\n",
    "\n",
    "4. **Error handling:**\n",
    "   - API failures logged per experiment\n",
    "   - One failed experiment doesn't halt the entire batch\n",
    "   - Can retry failed experiments later\n",
    "\n",
    "**Performance tuning:**\n",
    "- `max_workers=3`: Concurrent experiments (adjust for API rate limits)\n",
    "- `batch_size=None`: Process ALL pending experiments\n",
    "- Consider reducing `max_workers` if hitting rate limits across multiple API keys\n",
    "\n",
    "**Cost Warning:** \n",
    "- This executes experiments for **ALL models and populations**\n",
    "- Total API calls = (# of experiments) × (~50 questions) × (# of groups)\n",
    "- Verify API quotas for ALL models (OpenAI, Bedrock, etc.) before proceeding\n",
    "- Each model incurs its own API costs\n",
    "\n",
    "**Execution time estimate:**\n",
    "- GPT-4o: ~30-60 seconds per experiment\n",
    "- GPT-3.5: ~20-40 seconds per experiment  \n",
    "- Claude/Llama: Variable based on Bedrock configuration\n",
    "- Total time = (total experiments) / max_workers × avg_time_per_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Experiment Execution Configuration:\n",
      "   • Number of experiment groups: 3\n",
      "   • Total pending experiments: 6\n",
      "   • Max workers: 3\n",
      "   • Batch size: All\n",
      "\n",
      "📋 Experiment groups to execute:\n",
      "   • Group 1: gpt35 / generated_gpt35_spain826 (2 experiments)\n",
      "   • Group 2: gpt4o / borderline_maxN_gpt4o (2 experiments)\n",
      "   • Group 3: gpt4o / borderline_maxP_gpt4o (2 experiments)\n",
      "\n",
      "⚠️  Warning: This will make LLM API calls and may incur costs.\n",
      "   • Total experiments: 6\n",
      "   • Estimated API calls: 6 experiments × ~50 questions = ~300 calls\n",
      "   • Check your API quotas and rate limits before proceeding.\n",
      "\n",
      "⏳ Starting experiment execution for ALL groups...\n",
      "   Monitor logs for progress and errors.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:23:00,440 - evaluate_questionnaire_runner - INFO - Logger evaluate_questionnaire_runner is set up and ready to log.\n",
      "2025-10-09 18:23:00,440 - evaluate_questionnaire_runner - INFO - Logger evaluate_questionnaire_runner is set up and ready to log.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:23:00,518 - evaluate_questionnaire_runner - INFO - Connected to the database!\n",
      "2025-10-09 18:23:00,518 - evaluate_questionnaire_runner - INFO - Connected to the database!\n",
      "2025-10-09 18:23:00,641 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 3\n",
      "2025-10-09 18:23:00,641 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 3\n",
      "2025-10-09 18:23:00,646 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:00,646 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:00,650 - evaluate_questionnaire_runner - INFO - Model: gpt4o, Population: borderline_maxP_gpt4o, Questionnaire: epqra\n",
      "2025-10-09 18:23:00,650 - evaluate_questionnaire_runner - INFO - Model: gpt4o, Population: borderline_maxP_gpt4o, Questionnaire: epqra\n",
      "2025-10-09 18:23:00,918 - evaluate_questionnaire_runner - INFO - Submitting experiment 6 for personality 1\n",
      "2025-10-09 18:23:00,918 - evaluate_questionnaire_runner - INFO - Submitting experiment 6 for personality 1\n",
      "2025-10-09 18:23:00,920 - evaluate_questionnaire_runner - INFO - Submitting experiment 5 for personality 2\n",
      "2025-10-09 18:23:00,920 - evaluate_questionnaire_runner - INFO - Submitting experiment 5 for personality 2\n",
      "2025-10-09 18:23:02,070 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:02,070 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:02,921 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:02,921 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:06,594 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:06,609 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:06,609 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:06,611 - evaluate_questionnaire_runner - INFO - Input tokens: 686\n",
      "2025-10-09 18:23:06,611 - evaluate_questionnaire_runner - INFO - Input tokens: 686\n",
      "2025-10-09 18:23:06,618 - evaluate_questionnaire_runner - INFO - Output tokens: 238\n",
      "2025-10-09 18:23:06,618 - evaluate_questionnaire_runner - INFO - Output tokens: 238\n",
      "2025-10-09 18:23:06,623 - evaluate_questionnaire_runner - INFO - Total tokens: 924\n",
      "2025-10-09 18:23:06,623 - evaluate_questionnaire_runner - INFO - Total tokens: 924\n",
      "2025-10-09 18:23:06,629 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:06,629 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:06,638 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:06,638 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:06,711 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:06.710994 - Sending to DB, experiment_id: 5\n",
      "2025-10-09 18:23:06,711 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:06.710994 - Sending to DB, experiment_id: 5\n",
      "2025-10-09 18:23:06,754 - evaluate_questionnaire_runner - INFO - Updated experiment 5 status to succeeded\n",
      "2025-10-09 18:23:06,754 - evaluate_questionnaire_runner - INFO - Updated experiment 5 status to succeeded\n",
      "2025-10-09 18:23:06,762 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:06,762 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:08,843 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:08,846 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:08,846 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:08,850 - evaluate_questionnaire_runner - INFO - Input tokens: 726\n",
      "2025-10-09 18:23:08,850 - evaluate_questionnaire_runner - INFO - Input tokens: 726\n",
      "2025-10-09 18:23:08,854 - evaluate_questionnaire_runner - INFO - Output tokens: 246\n",
      "2025-10-09 18:23:08,854 - evaluate_questionnaire_runner - INFO - Output tokens: 246\n",
      "2025-10-09 18:23:08,862 - evaluate_questionnaire_runner - INFO - Total tokens: 972\n",
      "2025-10-09 18:23:08,862 - evaluate_questionnaire_runner - INFO - Total tokens: 972\n",
      "2025-10-09 18:23:08,867 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:08,867 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:08,871 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:08,871 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:08,913 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:08.913389 - Sending to DB, experiment_id: 6\n",
      "2025-10-09 18:23:08,913 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:08.913389 - Sending to DB, experiment_id: 6\n",
      "2025-10-09 18:23:08,948 - evaluate_questionnaire_runner - INFO - Updated experiment 6 status to succeeded\n",
      "2025-10-09 18:23:08,948 - evaluate_questionnaire_runner - INFO - Updated experiment 6 status to succeeded\n",
      "2025-10-09 18:23:08,956 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:08,956 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:09,011 - evaluate_questionnaire_runner - INFO - Updated 0 experiment groups (3-3) as concluded\n",
      "2025-10-09 18:23:09,011 - evaluate_questionnaire_runner - INFO - Updated 0 experiment groups (3-3) as concluded\n",
      "2025-10-09 18:23:09,019 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 3\n",
      "2025-10-09 18:23:09,019 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 3\n",
      "2025-10-09 18:23:09,028 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 2\n",
      "2025-10-09 18:23:09,028 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 2\n",
      "2025-10-09 18:23:09,041 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:09,041 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:09,050 - evaluate_questionnaire_runner - INFO - Model: gpt4o, Population: borderline_maxN_gpt4o, Questionnaire: epqra\n",
      "2025-10-09 18:23:09,050 - evaluate_questionnaire_runner - INFO - Model: gpt4o, Population: borderline_maxN_gpt4o, Questionnaire: epqra\n",
      "2025-10-09 18:23:09,155 - evaluate_questionnaire_runner - INFO - Submitting experiment 4 for personality 1\n",
      "2025-10-09 18:23:09,155 - evaluate_questionnaire_runner - INFO - Submitting experiment 4 for personality 1\n",
      "2025-10-09 18:23:09,159 - evaluate_questionnaire_runner - INFO - Submitting experiment 3 for personality 2\n",
      "2025-10-09 18:23:09,159 - evaluate_questionnaire_runner - INFO - Submitting experiment 3 for personality 2\n",
      "2025-10-09 18:23:10,634 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:10,634 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:11,097 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:11,097 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-4o-2024-11-20\n",
      "2025-10-09 18:23:14,684 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:14,688 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:14,688 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:14,691 - evaluate_questionnaire_runner - INFO - Input tokens: 742\n",
      "2025-10-09 18:23:14,691 - evaluate_questionnaire_runner - INFO - Input tokens: 742\n",
      "2025-10-09 18:23:14,694 - evaluate_questionnaire_runner - INFO - Output tokens: 231\n",
      "2025-10-09 18:23:14,694 - evaluate_questionnaire_runner - INFO - Output tokens: 231\n",
      "2025-10-09 18:23:14,700 - evaluate_questionnaire_runner - INFO - Total tokens: 973\n",
      "2025-10-09 18:23:14,700 - evaluate_questionnaire_runner - INFO - Total tokens: 973\n",
      "2025-10-09 18:23:14,706 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:14,706 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:14,712 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:14,712 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:14,744 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:14.744812 - Sending to DB, experiment_id: 4\n",
      "2025-10-09 18:23:14,744 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:14.744812 - Sending to DB, experiment_id: 4\n",
      "2025-10-09 18:23:14,770 - evaluate_questionnaire_runner - INFO - Updated experiment 4 status to succeeded\n",
      "2025-10-09 18:23:14,770 - evaluate_questionnaire_runner - INFO - Updated experiment 4 status to succeeded\n",
      "2025-10-09 18:23:14,778 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:14,778 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:15,301 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:15,305 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:15,305 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:15,308 - evaluate_questionnaire_runner - INFO - Input tokens: 746\n",
      "2025-10-09 18:23:15,308 - evaluate_questionnaire_runner - INFO - Input tokens: 746\n",
      "2025-10-09 18:23:15,313 - evaluate_questionnaire_runner - INFO - Output tokens: 228\n",
      "2025-10-09 18:23:15,313 - evaluate_questionnaire_runner - INFO - Output tokens: 228\n",
      "2025-10-09 18:23:15,320 - evaluate_questionnaire_runner - INFO - Total tokens: 974\n",
      "2025-10-09 18:23:15,320 - evaluate_questionnaire_runner - INFO - Total tokens: 974\n",
      "2025-10-09 18:23:15,332 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:15,332 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:15,340 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:15,340 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:15,398 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:15.397974 - Sending to DB, experiment_id: 3\n",
      "2025-10-09 18:23:15,398 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:15.397974 - Sending to DB, experiment_id: 3\n",
      "2025-10-09 18:23:15,445 - evaluate_questionnaire_runner - INFO - Updated experiment 3 status to succeeded\n",
      "2025-10-09 18:23:15,445 - evaluate_questionnaire_runner - INFO - Updated experiment 3 status to succeeded\n",
      "2025-10-09 18:23:15,455 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:15,455 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:15,487 - evaluate_questionnaire_runner - INFO - Updated 0 experiment groups (2-2) as concluded\n",
      "2025-10-09 18:23:15,487 - evaluate_questionnaire_runner - INFO - Updated 0 experiment groups (2-2) as concluded\n",
      "2025-10-09 18:23:15,495 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 2\n",
      "2025-10-09 18:23:15,495 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 2\n",
      "2025-10-09 18:23:15,502 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 1\n",
      "2025-10-09 18:23:15,502 - evaluate_questionnaire_runner - INFO - Processing experiments for experiments_group_id 1\n",
      "2025-10-09 18:23:15,511 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:15,511 - evaluate_questionnaire_runner - INFO - Found 2 experiments to process\n",
      "2025-10-09 18:23:15,517 - evaluate_questionnaire_runner - INFO - Model: gpt35, Population: generated_gpt35_spain826, Questionnaire: epqra\n",
      "2025-10-09 18:23:15,517 - evaluate_questionnaire_runner - INFO - Model: gpt35, Population: generated_gpt35_spain826, Questionnaire: epqra\n",
      "2025-10-09 18:23:15,607 - evaluate_questionnaire_runner - INFO - Submitting experiment 2 for personality 2\n",
      "2025-10-09 18:23:15,607 - evaluate_questionnaire_runner - INFO - Submitting experiment 2 for personality 2\n",
      "2025-10-09 18:23:15,609 - evaluate_questionnaire_runner - INFO - Submitting experiment 1 for personality 1\n",
      "2025-10-09 18:23:15,609 - evaluate_questionnaire_runner - INFO - Submitting experiment 1 for personality 1\n",
      "2025-10-09 18:23:16,680 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-3.5-turbo-0125\n",
      "2025-10-09 18:23:16,680 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-3.5-turbo-0125\n",
      "2025-10-09 18:23:16,992 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-3.5-turbo-0125\n",
      "2025-10-09 18:23:16,992 - evaluate_questionnaire_runner - INFO - Generating message with model gpt-3.5-turbo-0125\n",
      "2025-10-09 18:23:19,401 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:19,409 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:19,409 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:19,413 - evaluate_questionnaire_runner - INFO - Input tokens: 709\n",
      "2025-10-09 18:23:19,413 - evaluate_questionnaire_runner - INFO - Input tokens: 709\n",
      "2025-10-09 18:23:19,417 - evaluate_questionnaire_runner - INFO - Output tokens: 194\n",
      "2025-10-09 18:23:19,417 - evaluate_questionnaire_runner - INFO - Output tokens: 194\n",
      "2025-10-09 18:23:19,420 - evaluate_questionnaire_runner - INFO - Total tokens: 903\n",
      "2025-10-09 18:23:19,420 - evaluate_questionnaire_runner - INFO - Total tokens: 903\n",
      "2025-10-09 18:23:19,424 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:19,424 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:19,430 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:19,430 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:19,477 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:19.477317 - Sending to DB, experiment_id: 2\n",
      "2025-10-09 18:23:19,477 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:19.477317 - Sending to DB, experiment_id: 2\n",
      "2025-10-09 18:23:19,530 - evaluate_questionnaire_runner - INFO - Updated experiment 2 status to succeeded\n",
      "2025-10-09 18:23:19,530 - evaluate_questionnaire_runner - INFO - Updated experiment 2 status to succeeded\n",
      "2025-10-09 18:23:19,535 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:19,535 - evaluate_questionnaire_runner - INFO - Completed experiment 1/2\n",
      "2025-10-09 18:23:22,055 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 18:23:22,059 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:22,059 - evaluate_questionnaire_runner - INFO - Stop reason: valid\n",
      "2025-10-09 18:23:22,062 - evaluate_questionnaire_runner - INFO - Input tokens: 721\n",
      "2025-10-09 18:23:22,062 - evaluate_questionnaire_runner - INFO - Input tokens: 721\n",
      "2025-10-09 18:23:22,065 - evaluate_questionnaire_runner - INFO - Output tokens: 209\n",
      "2025-10-09 18:23:22,065 - evaluate_questionnaire_runner - INFO - Output tokens: 209\n",
      "2025-10-09 18:23:22,074 - evaluate_questionnaire_runner - INFO - Total tokens: 930\n",
      "2025-10-09 18:23:22,074 - evaluate_questionnaire_runner - INFO - Total tokens: 930\n",
      "2025-10-09 18:23:22,079 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:22,079 - evaluate_questionnaire_runner - INFO - Content is valid\n",
      "2025-10-09 18:23:22,082 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "2025-10-09 18:23:22,082 - evaluate_questionnaire_runner - INFO - Response is valid JSON, skipping additional processing\n",
      "/home/gferreir/mines/UOC/the_personality_trap/src/personas_backend/questionnaire/base_questionnaire.py:161: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"answer\"] = df.answer.replace(self.RESPONSE_MAPPING).infer_objects()\n",
      "2025-10-09 18:23:22,119 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:22.119629 - Sending to DB, experiment_id: 1\n",
      "2025-10-09 18:23:22,119 - evaluate_questionnaire_runner - INFO - 2025-10-09 18:23:22.119629 - Sending to DB, experiment_id: 1\n",
      "2025-10-09 18:23:22,161 - evaluate_questionnaire_runner - INFO - Updated experiment 1 status to succeeded\n",
      "2025-10-09 18:23:22,161 - evaluate_questionnaire_runner - INFO - Updated experiment 1 status to succeeded\n",
      "2025-10-09 18:23:22,167 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:22,167 - evaluate_questionnaire_runner - INFO - Completed experiment 2/2\n",
      "2025-10-09 18:23:22,192 - evaluate_questionnaire_runner - INFO - Updated 1 experiment groups (1-1) as concluded\n",
      "2025-10-09 18:23:22,192 - evaluate_questionnaire_runner - INFO - Updated 1 experiment groups (1-1) as concluded\n",
      "2025-10-09 18:23:22,197 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 1\n",
      "2025-10-09 18:23:22,197 - evaluate_questionnaire_runner - INFO - Finished processing experiment group 1\n",
      "2025-10-09 18:23:22,207 - evaluate_questionnaire_runner - INFO - Database connection closed successfully!\n",
      "2025-10-09 18:23:22,207 - evaluate_questionnaire_runner - INFO - Database connection closed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All experiment groups execution completed!\n",
      "   • Processed 3 experiment groups\n",
      "   • Total experiments: 6\n",
      "   Check the verification cells below for results.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for experiment execution\n",
    "MAX_WORKERS = 3  # Concurrent experiment workers (adjust based on API rate limits)\n",
    "BATCH_SIZE = None  # None = process all; set to integer to limit batch size\n",
    "\n",
    "# Get all experiment group IDs\n",
    "all_group_ids = [group['group_id'] for group in ALL_EXPERIMENT_GROUPS]\n",
    "\n",
    "print(f\"🎯 Experiment Execution Configuration:\")\n",
    "print(f\"   • Number of experiment groups: {len(all_group_ids)}\")\n",
    "print(f\"   • Total pending experiments: {len(ALL_REGISTERED_EXPERIMENTS)}\")\n",
    "print(f\"   • Max workers: {MAX_WORKERS}\")\n",
    "print(f\"   • Batch size: {'All' if BATCH_SIZE is None else BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\n📋 Experiment groups to execute:\")\n",
    "for group in ALL_EXPERIMENT_GROUPS:\n",
    "    print(f\"   • Group {group['group_id']}: {group['model']} / {group['population']} ({group['experiment_count']} experiments)\")\n",
    "\n",
    "# Confirm execution\n",
    "print(f\"\\n⚠️  Warning: This will make LLM API calls and may incur costs.\")\n",
    "print(f\"   • Total experiments: {len(ALL_REGISTERED_EXPERIMENTS)}\")\n",
    "print(f\"   • Estimated API calls: {len(ALL_REGISTERED_EXPERIMENTS)} experiments × ~50 questions = ~{len(ALL_REGISTERED_EXPERIMENTS) * 50} calls\")\n",
    "print(f\"   • Check your API quotas and rate limits before proceeding.\")\n",
    "\n",
    "CONFIRM_EXECUTION = True  # Set to True after reviewing the warning\n",
    "\n",
    "if not CONFIRM_EXECUTION:\n",
    "    print(f\"\\n⏸️  Execution paused. Set CONFIRM_EXECUTION=True to proceed.\")\n",
    "else:\n",
    "    print(f\"\\n⏳ Starting experiment execution for ALL groups...\")\n",
    "    print(f\"   Monitor logs for progress and errors.\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Execute all experiment groups in one batch\n",
    "        run_pending_experiments(\n",
    "            experiments_group_ids=all_group_ids,  # Pass ALL group IDs\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_workers=MAX_WORKERS,\n",
    "            schema=EXPERIMENTAL_SCHEMA,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ All experiment groups execution completed!\")\n",
    "        print(f\"   • Processed {len(all_group_ids)} experiment groups\")\n",
    "        print(f\"   • Total experiments: {len(ALL_REGISTERED_EXPERIMENTS)}\")\n",
    "        print(f\"   Check the verification cells below for results.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Experiment execution failed: {e}\")\n",
    "        print(f\"   Common issues:\")\n",
    "        print(f\"   • API key missing or invalid\")\n",
    "        print(f\"   • API rate limit exceeded\")\n",
    "        print(f\"   • Network connectivity issues\")\n",
    "        print(f\"   • LLM response parsing errors\")\n",
    "        print(f\"\\n   Check logs for detailed error messages.\")\n",
    "        print(f\"   Failed experiments can be retried by re-running this cell.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify All Experiment Completions Across Groups\n",
    "\n",
    "After execution, check results for **all experiment groups** across all models and populations. This helps identify:\n",
    "\n",
    "- **Overall success rate** across all experiments\n",
    "- **Per-group success rates** (model-population breakdown)\n",
    "- Failed experiments by model (API issues, parsing failures, etc.)\n",
    "- Pending experiments that need retry\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "**Overall Metrics:**\n",
    "- Total experiments executed across all groups\n",
    "- Success percentage (should be >95% ideally)\n",
    "- Distribution of failures by model\n",
    "\n",
    "**Per-Group Breakdown:**\n",
    "- Success rate for each model-population combination\n",
    "- Identify which models/populations had issues\n",
    "- Compare baseline vs borderline completion rates\n",
    "\n",
    "**Interpretation:**\n",
    "- `succeeded=TRUE`: Experiment completed, answers stored\n",
    "- `succeeded=FALSE`: Experiment failed, check `llm_explanation` for reason\n",
    "- `succeeded=NULL`: Not yet executed (retry needed)\n",
    "\n",
    "**Common patterns:**\n",
    "- GPT models usually have higher success rates\n",
    "- Borderline personas may have slightly higher failure rates (extreme responses)\n",
    "- Rate limit errors appear as clusters of failures in specific time windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Experiment Completion\n",
    "\n",
    "Confirm that the experiments completed successfully and inspect any reported errors. This cell shows:\n",
    "\n",
    "- Success rate summary\n",
    "- Status of each experiment\n",
    "- LLM explanations for completed experiments\n",
    "- Failed experiments (if any) for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Overall Experiment Status Summary:\n",
      "   • Total experiments: 6\n",
      "   • ✅ Succeeded: 6 (100.0%)\n",
      "   • ❌ Failed: 0 (0.0%)\n",
      "   • ⏳ Pending: 0 (0.0%)\n",
      "\n",
      "📋 Status by Experiment Group:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('experiments_group_id', 'model', 'population')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "total",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "succeeded",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "failed",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pending",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "success_rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b66a5e02-7fe7-44cc-a411-4a753e3e066b",
       "rows": [
        [
         "(np.int64(1), 'gpt-3.5-turbo-0125', 'generated_gpt35_spain826')",
         "2",
         "2",
         "0",
         "0",
         "100.0"
        ],
        [
         "(np.int64(2), 'gpt-4o-2024-11-20', 'borderline_maxN_gpt4o')",
         "2",
         "2",
         "0",
         "0",
         "100.0"
        ],
        [
         "(np.int64(3), 'gpt-4o-2024-11-20', 'borderline_maxP_gpt4o')",
         "2",
         "2",
         "0",
         "0",
         "100.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>succeeded</th>\n",
       "      <th>failed</th>\n",
       "      <th>pending</th>\n",
       "      <th>success_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <th>generated_gpt35_spain826</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxN_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxP_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  total  \\\n",
       "experiments_group_id model              population                        \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826      2   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o         2   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o         2   \n",
       "\n",
       "                                                                  succeeded  \\\n",
       "experiments_group_id model              population                            \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826          2   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o             2   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o             2   \n",
       "\n",
       "                                                                  failed  \\\n",
       "experiments_group_id model              population                         \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826       0   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o          0   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o          0   \n",
       "\n",
       "                                                                  pending  \\\n",
       "experiments_group_id model              population                          \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826        0   \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o           0   \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o           0   \n",
       "\n",
       "                                                                  success_rate  \n",
       "experiments_group_id model              population                              \n",
       "1                    gpt-3.5-turbo-0125 generated_gpt35_spain826         100.0  \n",
       "2                    gpt-4o-2024-11-20  borderline_maxN_gpt4o            100.0  \n",
       "3                    gpt-4o-2024-11-20  borderline_maxP_gpt4o            100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Sample Experiment Status (first 20 across all groups):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments_group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "personality_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "succeeded",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "76983877-482c-42de-a6e4-6fa3bf0f4e98",
       "rows": [
        [
         "0",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "1",
         "True"
        ],
        [
         "1",
         "1",
         "2",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "2",
         "True"
        ],
        [
         "2",
         "2",
         "3",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "2",
         "True"
        ],
        [
         "3",
         "2",
         "4",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "1",
         "True"
        ],
        [
         "4",
         "3",
         "5",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "2",
         "True"
        ],
        [
         "5",
         "3",
         "6",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "1",
         "True"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>personality_id</th>\n",
       "      <th>succeeded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiments_group_id  experiment_id               model  \\\n",
       "0                     1              1  gpt-3.5-turbo-0125   \n",
       "1                     1              2  gpt-3.5-turbo-0125   \n",
       "2                     2              3   gpt-4o-2024-11-20   \n",
       "3                     2              4   gpt-4o-2024-11-20   \n",
       "4                     3              5   gpt-4o-2024-11-20   \n",
       "5                     3              6   gpt-4o-2024-11-20   \n",
       "\n",
       "                 population  personality_id  succeeded  \n",
       "0  generated_gpt35_spain826               1       True  \n",
       "1  generated_gpt35_spain826               2       True  \n",
       "2     borderline_maxN_gpt4o               2       True  \n",
       "3     borderline_maxN_gpt4o               1       True  \n",
       "4     borderline_maxP_gpt4o               2       True  \n",
       "5     borderline_maxP_gpt4o               1       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query experiment status for ALL experiment groups\n",
    "all_group_ids_str = ', '.join(str(g['group_id']) for g in ALL_EXPERIMENT_GROUPS)\n",
    "\n",
    "status_query = f\"\"\"\n",
    "    SELECT\n",
    "        el.experiments_group_id,\n",
    "        el.experiment_id,\n",
    "        el.questionnaire,\n",
    "        el.model,\n",
    "        el.population,\n",
    "        el.personality_id,\n",
    "        el.repeated,\n",
    "        el.succeeded,\n",
    "        el.llm_explanation,\n",
    "        el.created\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.experiments_list el\n",
    "    WHERE el.experiments_group_id IN ({all_group_ids_str})\n",
    "    ORDER BY el.experiments_group_id, el.experiment_id\n",
    "\"\"\"\n",
    "\n",
    "experiment_status = pd.read_sql_query(status_query, db_handler.connection)\n",
    "\n",
    "# Overall summary statistics\n",
    "total_experiments = len(experiment_status)\n",
    "succeeded_count = (experiment_status['succeeded'] == True).sum()\n",
    "failed_count = (experiment_status['succeeded'] == False).sum()\n",
    "pending_count = experiment_status['succeeded'].isna().sum()\n",
    "\n",
    "print(f\"📊 Overall Experiment Status Summary:\")\n",
    "print(f\"   • Total experiments: {total_experiments}\")\n",
    "print(f\"   • ✅ Succeeded: {succeeded_count} ({succeeded_count/total_experiments*100:.1f}%)\")\n",
    "print(f\"   • ❌ Failed: {failed_count} ({failed_count/total_experiments*100:.1f}%)\")\n",
    "print(f\"   • ⏳ Pending: {pending_count} ({pending_count/total_experiments*100:.1f}%)\")\n",
    "\n",
    "# Per-group summary\n",
    "print(f\"\\n📋 Status by Experiment Group:\")\n",
    "group_summary = experiment_status.groupby(['experiments_group_id', 'model', 'population']).agg({\n",
    "    'experiment_id': 'count',\n",
    "    'succeeded': lambda x: (x == True).sum()\n",
    "}).rename(columns={'experiment_id': 'total', 'succeeded': 'succeeded'})\n",
    "group_summary['failed'] = experiment_status.groupby(['experiments_group_id', 'model', 'population'])['succeeded'].apply(lambda x: (x == False).sum())\n",
    "group_summary['pending'] = experiment_status.groupby(['experiments_group_id', 'model', 'population'])['succeeded'].apply(lambda x: x.isna().sum())\n",
    "group_summary['success_rate'] = (group_summary['succeeded'] / group_summary['total'] * 100).round(1)\n",
    "\n",
    "display(group_summary)\n",
    "\n",
    "# Display sample results from all groups\n",
    "print(f\"\\n📝 Sample Experiment Status (first 20 across all groups):\")\n",
    "display(experiment_status[['experiments_group_id', 'experiment_id', 'model', 'population', 'personality_id', 'succeeded']].head(20))\n",
    "\n",
    "# Show failed experiments if any\n",
    "if failed_count > 0:\n",
    "    print(f\"\\n⚠️  Failed Experiments:\")\n",
    "    failed_experiments = experiment_status[experiment_status['succeeded'] == False]\n",
    "    display(failed_experiments[['experiments_group_id', 'experiment_id', 'model', 'population', 'personality_id', 'llm_explanation']])\n",
    "    \n",
    "    print(f\"\\nℹ️  You can retry failed experiments by re-running the execution cell.\")\n",
    "\n",
    "# Check if all experiments completed\n",
    "if pending_count > 0:\n",
    "    print(f\"\\n⚠️  {pending_count} experiments still pending!\")\n",
    "    print(f\"   Re-run the execution cell (Step 5) to process them.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Review Questionnaire Answers Across All Models\n",
    "\n",
    "Inspect questionnaire responses collected from **all LLMs across all experiment groups**. This comprehensive view shows:\n",
    "\n",
    "- **Answer distribution** by model and population\n",
    "- **Answer completeness** (questions per experiment)\n",
    "- **Cross-model comparisons** of response patterns\n",
    "- **Baseline vs borderline** response differences\n",
    "\n",
    "**Database aggregation:**\n",
    "- Joins `eval_questionnaires` + `experiments_list` for full context\n",
    "- Groups by model, population, experiment group\n",
    "- Enables cross-model analysis\n",
    "\n",
    "**Key analyses:**\n",
    "\n",
    "1. **Model-Population Breakdown**\n",
    "   - How many answers per model-population combination\n",
    "   - Average completion rate by model\n",
    "   - Identify missing data\n",
    "\n",
    "2. **Answer Distribution (1-5 scale)**\n",
    "   - Overall response patterns across all models\n",
    "   - Model-specific biases (e.g., GPT-4o vs GPT-3.5)\n",
    "   - Baseline vs borderline response differences\n",
    "\n",
    "3. **Completeness Validation**\n",
    "   - Expected questions: 50 (Big Five) or 24 (EPQ-R-A)\n",
    "   - Detect incomplete experiments\n",
    "   - Flag parsing errors\n",
    "\n",
    "**Use for analysis:**\n",
    "- Calculate Big Five personality scores per model\n",
    "- Compare trait distributions across models\n",
    "- Analyze how different LLMs interpret the same persona\n",
    "- Study borderline condition effects on responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 144 questionnaire answers\n",
      "   • From 6 experiments\n",
      "   • Across 3 experiment groups\n",
      "   • Average answers per experiment: 24.0\n",
      "\n",
      "📊 Answers by Model and Population:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('model', 'population')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "total_answers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_answers",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "edaa168e-f4fb-4c89-b1e7-bf4564f91e66",
       "rows": [
        [
         "('gpt-3.5-turbo-0125', 'generated_gpt35_spain826')",
         "48",
         "2",
         "24.0"
        ],
        [
         "('gpt-4o-2024-11-20', 'borderline_maxN_gpt4o')",
         "48",
         "2",
         "24.0"
        ],
        [
         "('gpt-4o-2024-11-20', 'borderline_maxP_gpt4o')",
         "48",
         "2",
         "24.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_answers</th>\n",
       "      <th>experiments</th>\n",
       "      <th>avg_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <th>generated_gpt35_spain826</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxN_gpt4o</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borderline_maxP_gpt4o</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             total_answers  experiments  \\\n",
       "model              population                                             \n",
       "gpt-3.5-turbo-0125 generated_gpt35_spain826             48            2   \n",
       "gpt-4o-2024-11-20  borderline_maxN_gpt4o                48            2   \n",
       "                   borderline_maxP_gpt4o                48            2   \n",
       "\n",
       "                                             avg_answers  \n",
       "model              population                             \n",
       "gpt-3.5-turbo-0125 generated_gpt35_spain826         24.0  \n",
       "gpt-4o-2024-11-20  borderline_maxN_gpt4o            24.0  \n",
       "                   borderline_maxP_gpt4o            24.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Overall Answer Distribution (1-5 scale):\n",
      "   • 0:    71 ( 49.3%)\n",
      "   • 1:    73 ( 50.7%)\n",
      "\n",
      "📋 Answer Completeness:\n",
      "   • Expected questions per experiment: 24\n",
      "   • Complete experiments: 6\n",
      "   • Incomplete experiments: 0\n",
      "\n",
      "📝 Sample Answers (first 20 rows across all groups):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments_group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answer",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9e93c831-b4af-4263-961a-53a36daa09c4",
       "rows": [
        [
         "0",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "1",
         "1"
        ],
        [
         "1",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "2",
         "0"
        ],
        [
         "2",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "3",
         "1"
        ],
        [
         "3",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "4",
         "0"
        ],
        [
         "4",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "5",
         "0"
        ],
        [
         "5",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "6",
         "0"
        ],
        [
         "6",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "7",
         "0"
        ],
        [
         "7",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "8",
         "1"
        ],
        [
         "8",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "9",
         "1"
        ],
        [
         "9",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "10",
         "0"
        ],
        [
         "10",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "11",
         "0"
        ],
        [
         "11",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "12",
         "0"
        ],
        [
         "12",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "13",
         "1"
        ],
        [
         "13",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "14",
         "1"
        ],
        [
         "14",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "15",
         "1"
        ],
        [
         "15",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "16",
         "1"
        ],
        [
         "16",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "17",
         "0"
        ],
        [
         "17",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "18",
         "0"
        ],
        [
         "18",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "19",
         "0"
        ],
        [
         "19",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "20",
         "1"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiments_group_id  experiment_id               model  \\\n",
       "0                      1              1  gpt-3.5-turbo-0125   \n",
       "1                      1              1  gpt-3.5-turbo-0125   \n",
       "2                      1              1  gpt-3.5-turbo-0125   \n",
       "3                      1              1  gpt-3.5-turbo-0125   \n",
       "4                      1              1  gpt-3.5-turbo-0125   \n",
       "5                      1              1  gpt-3.5-turbo-0125   \n",
       "6                      1              1  gpt-3.5-turbo-0125   \n",
       "7                      1              1  gpt-3.5-turbo-0125   \n",
       "8                      1              1  gpt-3.5-turbo-0125   \n",
       "9                      1              1  gpt-3.5-turbo-0125   \n",
       "10                     1              1  gpt-3.5-turbo-0125   \n",
       "11                     1              1  gpt-3.5-turbo-0125   \n",
       "12                     1              1  gpt-3.5-turbo-0125   \n",
       "13                     1              1  gpt-3.5-turbo-0125   \n",
       "14                     1              1  gpt-3.5-turbo-0125   \n",
       "15                     1              1  gpt-3.5-turbo-0125   \n",
       "16                     1              1  gpt-3.5-turbo-0125   \n",
       "17                     1              1  gpt-3.5-turbo-0125   \n",
       "18                     1              1  gpt-3.5-turbo-0125   \n",
       "19                     1              1  gpt-3.5-turbo-0125   \n",
       "\n",
       "                  population  question_number  answer  \n",
       "0   generated_gpt35_spain826                1       1  \n",
       "1   generated_gpt35_spain826                2       0  \n",
       "2   generated_gpt35_spain826                3       1  \n",
       "3   generated_gpt35_spain826                4       0  \n",
       "4   generated_gpt35_spain826                5       0  \n",
       "5   generated_gpt35_spain826                6       0  \n",
       "6   generated_gpt35_spain826                7       0  \n",
       "7   generated_gpt35_spain826                8       1  \n",
       "8   generated_gpt35_spain826                9       1  \n",
       "9   generated_gpt35_spain826               10       0  \n",
       "10  generated_gpt35_spain826               11       0  \n",
       "11  generated_gpt35_spain826               12       0  \n",
       "12  generated_gpt35_spain826               13       1  \n",
       "13  generated_gpt35_spain826               14       1  \n",
       "14  generated_gpt35_spain826               15       1  \n",
       "15  generated_gpt35_spain826               16       1  \n",
       "16  generated_gpt35_spain826               17       0  \n",
       "17  generated_gpt35_spain826               18       0  \n",
       "18  generated_gpt35_spain826               19       0  \n",
       "19  generated_gpt35_spain826               20       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get ALL experiment IDs across all groups\n",
    "all_experiment_ids_str = ', '.join(str(eid) for eid in ALL_REGISTERED_EXPERIMENTS)\n",
    "\n",
    "# Query questionnaire answers for ALL experiments\n",
    "answers_query = f\"\"\"\n",
    "    SELECT\n",
    "        eq.experiment_id,\n",
    "        eq.question_number,\n",
    "        eq.answer,\n",
    "        el.model,\n",
    "        el.population,\n",
    "        el.personality_id,\n",
    "        el.experiments_group_id\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.eval_questionnaires eq\n",
    "    JOIN {EXPERIMENTAL_SCHEMA}.experiments_list el\n",
    "        ON eq.experiment_id = el.experiment_id\n",
    "    WHERE eq.experiment_id IN ({all_experiment_ids_str})\n",
    "    ORDER BY el.experiments_group_id, eq.experiment_id, eq.question_number\n",
    "\"\"\"\n",
    "\n",
    "answers_df = pd.read_sql_query(answers_query, db_handler.connection)\n",
    "\n",
    "if answers_df.empty:\n",
    "    print(f\"⚠️  No answers found!\")\n",
    "    print(f\"   Possible reasons:\")\n",
    "    print(f\"   • Experiments haven't been executed yet\")\n",
    "    print(f\"   • All experiments failed (check status in previous cell)\")\n",
    "    print(f\"   • Database query error\")\n",
    "else:\n",
    "    total_answers = len(answers_df)\n",
    "    unique_experiments = answers_df['experiment_id'].nunique()\n",
    "    unique_groups = answers_df['experiments_group_id'].nunique()\n",
    "    avg_answers_per_experiment = total_answers / unique_experiments if unique_experiments > 0 else 0\n",
    "    \n",
    "    print(f\"✅ Found {total_answers} questionnaire answers\")\n",
    "    print(f\"   • From {unique_experiments} experiments\")\n",
    "    print(f\"   • Across {unique_groups} experiment groups\")\n",
    "    print(f\"   • Average answers per experiment: {avg_answers_per_experiment:.1f}\")\n",
    "    \n",
    "    # Answer distribution by model and population\n",
    "    print(f\"\\n📊 Answers by Model and Population:\")\n",
    "    model_pop_summary = answers_df.groupby(['model', 'population']).agg({\n",
    "        'answer': 'count',\n",
    "        'experiment_id': 'nunique'\n",
    "    }).rename(columns={'answer': 'total_answers', 'experiment_id': 'experiments'})\n",
    "    model_pop_summary['avg_answers'] = (model_pop_summary['total_answers'] / model_pop_summary['experiments']).round(1)\n",
    "    display(model_pop_summary)\n",
    "    \n",
    "    # Overall answer distribution analysis\n",
    "    print(f\"\\n📊 Overall Answer Distribution (1-5 scale):\")\n",
    "    answer_dist = answers_df['answer'].value_counts().sort_index()\n",
    "    for value, count in answer_dist.items():\n",
    "        percentage = count / total_answers * 100\n",
    "        print(f\"   • {value}: {count:5d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Check completeness dynamically based on most common answer count\n",
    "    answers_per_exp = answers_df.groupby('experiment_id').size()\n",
    "    expected_questions = answers_per_exp.mode()[0]  # Most common count\n",
    "    complete_experiments = (answers_per_exp == expected_questions).sum()\n",
    "    \n",
    "    print(f\"\\n📋 Answer Completeness:\")\n",
    "    print(f\"   • Expected questions per experiment: {expected_questions}\")\n",
    "    print(f\"   • Complete experiments: {complete_experiments}\")\n",
    "    print(f\"   • Incomplete experiments: {unique_experiments - complete_experiments}\")\n",
    "    \n",
    "    if (unique_experiments - complete_experiments) > 0:\n",
    "        print(f\"\\n⚠️  Some experiments have unexpected answer counts:\")\n",
    "        incomplete = answers_per_exp[answers_per_exp != expected_questions]\n",
    "        incomplete_df = pd.DataFrame({\n",
    "            'experiment_id': incomplete.index, \n",
    "            'answer_count': incomplete.values\n",
    "        })\n",
    "        # Add model/population info for context\n",
    "        incomplete_with_info = incomplete_df.merge(\n",
    "            answers_df[['experiment_id', 'model', 'population']].drop_duplicates(),\n",
    "            on='experiment_id'\n",
    "        )\n",
    "        display(incomplete_with_info)\n",
    "    \n",
    "    # Display sample answers from multiple groups\n",
    "    print(f\"\\n📝 Sample Answers (first 20 rows across all groups):\")\n",
    "    display(answers_df[['experiments_group_id', 'experiment_id', 'model', 'population', 'question_number', 'answer']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Answer Summary Statistics Across All Groups\n",
    "\n",
    "Aggregate answer statistics to understand the **complete dataset** across all models and populations:\n",
    "\n",
    "**Per-Experiment Summary:**\n",
    "- Answer counts for each experiment\n",
    "- Grouped by model and population\n",
    "- Identify experiments with missing answers\n",
    "\n",
    "**Overall Statistics:**\n",
    "- Total answers collected across all groups\n",
    "- Average answers per experiment\n",
    "- Min/max answer counts (quality check)\n",
    "\n",
    "**Model-Population Breakdown:**\n",
    "- Experiments completed per model-population pair\n",
    "- Total answers per combination\n",
    "- Average completeness rate\n",
    "\n",
    "**Quality Metrics:**\n",
    "- All experiments should have consistent answer counts\n",
    "- Big Five: 50 answers per experiment\n",
    "- EPQ-R-A: 24 answers per experiment\n",
    "- Outliers indicate parsing or API issues\n",
    "\n",
    "This summary helps:\n",
    "- Verify data completeness before analysis\n",
    "- Identify systematic issues with specific models\n",
    "- Confirm baseline and borderline populations are represented\n",
    "- Plan next steps for statistical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Answer Summary per Experiment (showing first 20):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiments_group_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "population",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "personality_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answers_recorded",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d9896c33-13f4-4e6d-80fc-50f580bb6055",
       "rows": [
        [
         "0",
         "1",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "1",
         "24"
        ],
        [
         "1",
         "2",
         "1",
         "gpt-3.5-turbo-0125",
         "generated_gpt35_spain826",
         "2",
         "24"
        ],
        [
         "2",
         "3",
         "2",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "2",
         "24"
        ],
        [
         "3",
         "4",
         "2",
         "gpt-4o-2024-11-20",
         "borderline_maxN_gpt4o",
         "1",
         "24"
        ],
        [
         "4",
         "5",
         "3",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "2",
         "24"
        ],
        [
         "5",
         "6",
         "3",
         "gpt-4o-2024-11-20",
         "borderline_maxP_gpt4o",
         "1",
         "24"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>experiments_group_id</th>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th>personality_id</th>\n",
       "      <th>answers_recorded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>generated_gpt35_spain826</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxN_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-4o-2024-11-20</td>\n",
       "      <td>borderline_maxP_gpt4o</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id  experiments_group_id               model  \\\n",
       "0              1                     1  gpt-3.5-turbo-0125   \n",
       "1              2                     1  gpt-3.5-turbo-0125   \n",
       "2              3                     2   gpt-4o-2024-11-20   \n",
       "3              4                     2   gpt-4o-2024-11-20   \n",
       "4              5                     3   gpt-4o-2024-11-20   \n",
       "5              6                     3   gpt-4o-2024-11-20   \n",
       "\n",
       "                 population  personality_id  answers_recorded  \n",
       "0  generated_gpt35_spain826               1                24  \n",
       "1  generated_gpt35_spain826               2                24  \n",
       "2     borderline_maxN_gpt4o               2                24  \n",
       "3     borderline_maxN_gpt4o               1                24  \n",
       "4     borderline_maxP_gpt4o               2                24  \n",
       "5     borderline_maxP_gpt4o               1                24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Statistics:\n",
      "   • Total experiments with answers: 6\n",
      "   • Total answers collected: 144\n",
      "   • Average answers per experiment: 24.0\n",
      "   • Min answers in any experiment: 24\n",
      "   • Max answers in any experiment: 24\n",
      "\n",
      "📊 Summary by Model and Population:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('model', 'population')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "experiments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_answers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_answers",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3c8efefd-a33e-42d7-9091-f6548610a0d9",
       "rows": [
        [
         "('gpt-3.5-turbo-0125', 'generated_gpt35_spain826')",
         "2",
         "48",
         "24.0"
        ],
        [
         "('gpt-4o-2024-11-20', 'borderline_maxN_gpt4o')",
         "2",
         "48",
         "24.0"
        ],
        [
         "('gpt-4o-2024-11-20', 'borderline_maxP_gpt4o')",
         "2",
         "48",
         "24.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>experiments</th>\n",
       "      <th>total_answers</th>\n",
       "      <th>avg_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>population</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <th>generated_gpt35_spain826</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt-4o-2024-11-20</th>\n",
       "      <th>borderline_maxN_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borderline_maxP_gpt4o</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             experiments  total_answers  \\\n",
       "model              population                                             \n",
       "gpt-3.5-turbo-0125 generated_gpt35_spain826            2             48   \n",
       "gpt-4o-2024-11-20  borderline_maxN_gpt4o               2             48   \n",
       "                   borderline_maxP_gpt4o               2             48   \n",
       "\n",
       "                                             avg_answers  \n",
       "model              population                             \n",
       "gpt-3.5-turbo-0125 generated_gpt35_spain826         24.0  \n",
       "gpt-4o-2024-11-20  borderline_maxN_gpt4o            24.0  \n",
       "                   borderline_maxP_gpt4o            24.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate answers by experiment across ALL groups\n",
    "summary_query = f\"\"\"\n",
    "    SELECT\n",
    "        eq.experiment_id,\n",
    "        el.experiments_group_id,\n",
    "        el.model,\n",
    "        el.population,\n",
    "        el.personality_id,\n",
    "        COUNT(*) AS answers_recorded\n",
    "    FROM {EXPERIMENTAL_SCHEMA}.eval_questionnaires eq\n",
    "    JOIN {EXPERIMENTAL_SCHEMA}.experiments_list el\n",
    "        ON eq.experiment_id = el.experiment_id\n",
    "    WHERE eq.experiment_id IN ({all_experiment_ids_str})\n",
    "    GROUP BY eq.experiment_id, el.experiments_group_id, el.model, el.population, el.personality_id\n",
    "    ORDER BY el.experiments_group_id, eq.experiment_id\n",
    "\"\"\"\n",
    "\n",
    "summary_df = pd.read_sql_query(summary_query, db_handler.connection)\n",
    "\n",
    "print(f\"📊 Answer Summary per Experiment (showing first 20):\")\n",
    "display(summary_df.head(20))\n",
    "\n",
    "if not summary_df.empty:\n",
    "    print(f\"\\n✅ Overall Statistics:\")\n",
    "    print(f\"   • Total experiments with answers: {len(summary_df)}\")\n",
    "    print(f\"   • Total answers collected: {summary_df['answers_recorded'].sum()}\")\n",
    "    print(f\"   • Average answers per experiment: {summary_df['answers_recorded'].mean():.1f}\")\n",
    "    print(f\"   • Min answers in any experiment: {summary_df['answers_recorded'].min()}\")\n",
    "    print(f\"   • Max answers in any experiment: {summary_df['answers_recorded'].max()}\")\n",
    "    \n",
    "    # Summary by model-population\n",
    "    print(f\"\\n📊 Summary by Model and Population:\")\n",
    "    model_pop_stats = summary_df.groupby(['model', 'population']).agg({\n",
    "        'experiment_id': 'count',\n",
    "        'answers_recorded': ['sum', 'mean']\n",
    "    })\n",
    "    model_pop_stats.columns = ['experiments', 'total_answers', 'avg_answers']\n",
    "    model_pop_stats['avg_answers'] = model_pop_stats['avg_answers'].round(1)\n",
    "    display(model_pop_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Inspect LLM Request/Response Metadata (Optional)\n",
    "\n",
    "For debugging or detailed analysis across **all models**, inspect raw LLM API calls stored in `experiment_request_metadata`.\n",
    "\n",
    "**Multi-Model Metadata:**\n",
    "- Request/response JSON for each model (GPT, Claude, Llama)\n",
    "- Token usage by model (important for cost tracking)\n",
    "- API call patterns and timing\n",
    "- Model-specific metadata fields\n",
    "\n",
    "**Use cases:**\n",
    "\n",
    "1. **Cost Analysis**\n",
    "   - Track token usage per model\n",
    "   - Compare GPT-4o vs GPT-3.5 efficiency\n",
    "   - Estimate future experiment costs\n",
    "\n",
    "2. **Debugging**\n",
    "   - Inspect failed experiment request/response pairs\n",
    "   - Compare successful vs failed API calls\n",
    "   - Identify model-specific parsing issues\n",
    "\n",
    "3. **Performance Analysis**\n",
    "   - Response times by model\n",
    "   - Token efficiency (tokens per answer)\n",
    "   - API throughput patterns\n",
    "\n",
    "4. **Research Reproducibility**\n",
    "   - Full API request history\n",
    "   - Exact prompts sent to each model\n",
    "   - Complete LLM responses for verification\n",
    "\n",
    "**Token usage insights:**\n",
    "- GPT-4o: Higher cost per token, better quality\n",
    "- GPT-3.5: Lower cost, faster responses\n",
    "- Claude/Llama: Variable based on Bedrock configuration\n",
    "\n",
    "**Note:** Enable `INSPECT_METADATA=True` to view detailed API metadata and token statistics by model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b446773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  Request/response metadata inspection skipped.\n",
      "   Set INSPECT_METADATA=True to view API call details and token usage by model.\n"
     ]
    }
   ],
   "source": [
    "# Query request/response metadata (summary only, not full JSON)\n",
    "INSPECT_METADATA = False  # Set to True to enable this optional inspection\n",
    "\n",
    "if INSPECT_METADATA:\n",
    "    metadata_query = f\"\"\"\n",
    "        SELECT\n",
    "            erm.id,\n",
    "            erm.experiment_id,\n",
    "            el.experiments_group_id,\n",
    "            el.model,\n",
    "            el.population,\n",
    "            erm.created,\n",
    "            jsonb_array_length(erm.request_json->'messages') as message_count,\n",
    "            erm.response_json->'usage'->>'total_tokens' as total_tokens,\n",
    "            erm.response_json->'usage'->>'prompt_tokens' as prompt_tokens,\n",
    "            erm.response_json->'usage'->>'completion_tokens' as completion_tokens\n",
    "        FROM {EXPERIMENTAL_SCHEMA}.experiment_request_metadata erm\n",
    "        JOIN {EXPERIMENTAL_SCHEMA}.experiments_list el\n",
    "            ON erm.experiment_id = el.experiment_id\n",
    "        WHERE erm.experiment_id IN ({all_experiment_ids_str})\n",
    "        ORDER BY erm.created DESC\n",
    "        LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata_df = pd.read_sql_query(metadata_query, db_handler.connection)\n",
    "    \n",
    "    if not metadata_df.empty:\n",
    "        print(f\"📡 LLM API Request/Response Metadata (sample - 20 most recent):\")\n",
    "        display(metadata_df)\n",
    "        \n",
    "        # Token usage summary by model\n",
    "        if 'total_tokens' in metadata_df.columns:\n",
    "            print(f\"\\n📊 Token Usage Summary:\")\n",
    "            token_summary = metadata_df.groupby('model').agg({\n",
    "                'total_tokens': lambda x: x.astype(float).sum(),\n",
    "                'prompt_tokens': lambda x: x.astype(float).sum(),\n",
    "                'completion_tokens': lambda x: x.astype(float).sum(),\n",
    "                'id': 'count'\n",
    "            }).rename(columns={'id': 'api_calls'})\n",
    "            \n",
    "            for col in ['total_tokens', 'prompt_tokens', 'completion_tokens']:\n",
    "                token_summary[col] = token_summary[col].astype(int)\n",
    "            \n",
    "            display(token_summary)\n",
    "            \n",
    "            total_tokens_all = metadata_df['total_tokens'].astype(float).sum()\n",
    "            print(f\"\\n   💰 Total tokens used (sampled): {total_tokens_all:,.0f}\")\n",
    "    else:\n",
    "        print(f\"⚠️  No request metadata found.\")\n",
    "else:\n",
    "    print(f\"ℹ️  Request/response metadata inspection skipped.\")\n",
    "    print(f\"   Set INSPECT_METADATA=True to view API call details and token usage by model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:23:22,697 - personas_backend.db.db_handler - INFO - Database connection closed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection closed\n",
      "\n",
      "🎉 Questionnaire experiments complete!\n",
      "\n",
      "📊 Summary:\n",
      "   • Experiment groups created: 3\n",
      "   • Total experiments executed: 6\n",
      "   • Questionnaire answers collected: 144\n",
      "   • Unique models tested: 2\n",
      "   • Unique populations tested: 3\n",
      "\n",
      "📋 Experiment Groups Summary:\n",
      "   • Group 1: gpt35 / generated_gpt35_spain826 (2 experiments)\n",
      "   • Group 2: gpt4o / borderline_maxN_gpt4o (2 experiments)\n",
      "   • Group 3: gpt4o / borderline_maxP_gpt4o (2 experiments)\n",
      "\n",
      "🔬 Next Steps for Analysis:\n",
      "   • Calculate Big Five personality scores from answers\n",
      "   • Compare LLM responses across different models\n",
      "   • Analyze baseline vs borderline persona responses\n",
      "   • Study demographic bias patterns across models and populations\n",
      "   • Export data for statistical analysis\n",
      "\n",
      "💾 Data Export Example:\n",
      "   # Export all answers with model/population info\n",
      "   # answers_df.to_csv('all_questionnaire_answers.csv', index=False)\n",
      "   # \n",
      "   # Export experiment status summary\n",
      "   # experiment_status.to_csv('all_experiment_status.csv', index=False)\n",
      "   # \n",
      "   # Export per-group summaries\n",
      "   # summary_df.to_csv('experiment_summaries.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "db_handler.close_connection()\n",
    "print(\"✅ Database connection closed\")\n",
    "\n",
    "print(f\"\\n🎉 Questionnaire experiments complete!\")\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"   • Experiment groups created: {len(ALL_EXPERIMENT_GROUPS)}\")\n",
    "print(f\"   • Total experiments executed: {len(ALL_REGISTERED_EXPERIMENTS)}\")\n",
    "\n",
    "if 'answers_df' in locals() and not answers_df.empty:\n",
    "    print(f\"   • Questionnaire answers collected: {len(answers_df)}\")\n",
    "    print(f\"   • Unique models tested: {answers_df['model'].nunique()}\")\n",
    "    print(f\"   • Unique populations tested: {answers_df['population'].nunique()}\")\n",
    "\n",
    "print(f\"\\n📋 Experiment Groups Summary:\")\n",
    "for group in ALL_EXPERIMENT_GROUPS:\n",
    "    print(f\"   • Group {group['group_id']}: {group['model']} / {group['population']} ({group['experiment_count']} experiments)\")\n",
    "\n",
    "print(f\"\\n🔬 Next Steps for Analysis:\")\n",
    "print(f\"   • Calculate Big Five personality scores from answers\")\n",
    "print(f\"   • Compare LLM responses across different models\")\n",
    "print(f\"   • Analyze baseline vs borderline persona responses\")\n",
    "print(f\"   • Study demographic bias patterns across models and populations\")\n",
    "print(f\"   • Export data for statistical analysis\")\n",
    "\n",
    "print(f\"\\n💾 Data Export Example:\")\n",
    "print(f\"   # Export all answers with model/population info\")\n",
    "print(f\"   # answers_df.to_csv('all_questionnaire_answers.csv', index=False)\")\n",
    "print(f\"   # \")\n",
    "print(f\"   # Export experiment status summary\")\n",
    "print(f\"   # experiment_status.to_csv('all_experiment_status.csv', index=False)\")\n",
    "print(f\"   # \")\n",
    "print(f\"   # Export per-group summaries\")\n",
    "print(f\"   # summary_df.to_csv('experiment_summaries.csv', index=False)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
